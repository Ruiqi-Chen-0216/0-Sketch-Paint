{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QuFz5uGi-h6G"
      },
      "outputs": [],
      "source": [
        "%pip install --quiet --upgrade diffusers transformers accelerate mediapy triton scipy ftfy spacy==3.5.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==1.13.1 torchvision==0.14.1 torchaudio==0.13.1 torchtext==0.14.1 fastai==2.7.11\n",
        "!pip install tokenizers\n",
        "#!pip install torchdata==0.5.1"
      ],
      "metadata": {
        "id": "W9fKOyi1TpIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U pillow"
      ],
      "metadata": {
        "id": "wGQt1h63XJyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The xformers package is mandatory to be able to create several 768x768 images.\n",
        "%pip install -q xformers==0.0.16rc425"
      ],
      "metadata": {
        "id": "oP_dBQpSCIkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install -q --pre xformers"
      ],
      "metadata": {
        "id": "4xq_sZtIPpmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_id = \"stabilityai/stable-diffusion-2-1-base\"\n",
        "# model_id = \"stabilityai/stable-diffusion-2-1\"\n",
        "model_id = \"CompVis/stable-diffusion-v1-4\"\n",
        "#\"dreamlike-art/dreamlike-photoreal-2.0\""
      ],
      "metadata": {
        "id": "GR4vF2bw-sHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import PNDMScheduler, DDIMScheduler, LMSDiscreteScheduler, EulerDiscreteScheduler, DPMSolverMultistepScheduler\n",
        "\n",
        "scheduler = None\n",
        "# scheduler = PNDMScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\n",
        "# scheduler = DDIMScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\n",
        "# scheduler = LMSDiscreteScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\n",
        "# scheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\n",
        "# scheduler = DPMSolverMultistepScheduler.from_pretrained(model_id, subfolder=\"scheduler\")"
      ],
      "metadata": {
        "id": "vF9Q0xKX8gLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import mediapy as media\n",
        "import torch\n",
        "from diffusers import StableDiffusionPipeline\n",
        "import random\n",
        "\n",
        "device = \"cuda\"\n",
        "\n",
        "if model_id.startswith(\"stabilityai/\"):\n",
        "  model_revision = \"fp16\"\n",
        "else:\n",
        "  model_revision = None\n",
        "\n",
        "if scheduler is None:\n",
        "  pipe = StableDiffusionPipeline.from_pretrained(\n",
        "      model_id,\n",
        "      torch_dtype=torch.float16,\n",
        "      revision=model_revision,\n",
        "      )  \n",
        "else:\n",
        "  pipe = StableDiffusionPipeline.from_pretrained(\n",
        "      model_id,\n",
        "      scheduler=scheduler,\n",
        "      torch_dtype=torch.float16,\n",
        "      revision=model_revision,\n",
        "      )\n",
        "\n",
        "pipe = pipe.to(device)\n",
        "pipe.enable_xformers_memory_efficient_attention()\n",
        "\n",
        "if model_id.endswith('-base'):\n",
        "  image_length = 512\n",
        "else:\n",
        "  image_length = 768\n"
      ],
      "metadata": {
        "id": "bG2hkmSEvByV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"A girl in a dress with flowers in hands\"\n",
        "remove_safety = False\n",
        "num_images = 4\n",
        "seed = random.randint(0, 2147483647)\n",
        "\n",
        "if remove_safety:\n",
        "  negative_prompt = None\n",
        "  pipe.safety_checker = None\n",
        "else:\n",
        "  negative_prompt = \"nude, naked\"\n",
        "\n",
        "images = pipe(\n",
        "    prompt,\n",
        "    height = image_length,\n",
        "    width = image_length,\n",
        "    num_inference_steps = 25,\n",
        "    guidance_scale = 9,\n",
        "    num_images_per_prompt = num_images,\n",
        "    negative_prompt = negative_prompt,\n",
        "    generator = torch.Generator(\"cuda\").manual_seed(seed)\n",
        "    ).images\n",
        "    \n",
        "media.show_images(images)\n",
        "display(f\"Seed: {seed}\")\n",
        "images[0].save(\"output.jpg\")"
      ],
      "metadata": {
        "id": "AUc4QJfE-uR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from safetensors.torch import load_file\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from diffusers import DPMSolverMultistepScheduler\n",
        "\n",
        "# load diffusers model\n",
        "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
        "pipeline = StableDiffusionPipeline.from_pretrained(model_id,torch_dtype=torch.float32)\n",
        "pipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config)\n",
        "\n",
        "# load lora weight\n",
        "model_path = \"animeoutlineV4_16.safetensors\"\n",
        "state_dict = load_file(model_path)\n",
        "\n",
        "LORA_PREFIX_UNET = 'lora_unet'\n",
        "LORA_PREFIX_TEXT_ENCODER = 'lora_te'\n",
        "\n",
        "alpha = 0.75\n",
        "\n",
        "visited = []\n",
        "\n",
        "# directly update weight in diffusers model\n",
        "for key in state_dict:\n",
        "    \n",
        "    # it is suggested to print out the key, it usually will be something like below\n",
        "    # \"lora_te_text_model_encoder_layers_0_self_attn_k_proj.lora_down.weight\"\n",
        "    \n",
        "    # as we have set the alpha beforehand, so just skip\n",
        "    if '.alpha' in key or key in visited:\n",
        "        continue\n",
        "        \n",
        "    if 'text' in key:\n",
        "        layer_infos = key.split('.')[0].split(LORA_PREFIX_TEXT_ENCODER+'_')[-1].split('_')\n",
        "        curr_layer = pipeline.text_encoder\n",
        "    else:\n",
        "        layer_infos = key.split('.')[0].split(LORA_PREFIX_UNET+'_')[-1].split('_')\n",
        "        curr_layer = pipeline.unet\n",
        "\n",
        "    # find the target layer\n",
        "    temp_name = layer_infos.pop(0)\n",
        "    while len(layer_infos) > -1:\n",
        "        try:\n",
        "            curr_layer = curr_layer.__getattr__(temp_name)\n",
        "            if len(layer_infos) > 0:\n",
        "                temp_name = layer_infos.pop(0)\n",
        "            elif len(layer_infos) == 0:\n",
        "                break\n",
        "        except Exception:\n",
        "            if len(temp_name) > 0:\n",
        "                temp_name += '_'+layer_infos.pop(0)\n",
        "            else:\n",
        "                temp_name = layer_infos.pop(0)\n",
        "    \n",
        "    # org_forward(x) + lora_up(lora_down(x)) * multiplier\n",
        "    pair_keys = []\n",
        "    if 'lora_down' in key:\n",
        "        pair_keys.append(key.replace('lora_down', 'lora_up'))\n",
        "        pair_keys.append(key)\n",
        "    else:\n",
        "        pair_keys.append(key)\n",
        "        pair_keys.append(key.replace('lora_up', 'lora_down'))\n",
        "    \n",
        "    # update weight\n",
        "    if len(state_dict[pair_keys[0]].shape) == 4:\n",
        "        weight_up = state_dict[pair_keys[0]].squeeze(3).squeeze(2).to(torch.float32)\n",
        "        weight_down = state_dict[pair_keys[1]].squeeze(3).squeeze(2).to(torch.float32)\n",
        "        curr_layer.weight.data += alpha * torch.mm(weight_up, weight_down).unsqueeze(2).unsqueeze(3)\n",
        "    else:\n",
        "        weight_up = state_dict[pair_keys[0]].to(torch.float32)\n",
        "        weight_down = state_dict[pair_keys[1]].to(torch.float32)\n",
        "        curr_layer.weight.data += alpha * torch.mm(weight_up, weight_down)\n",
        "        \n",
        "     # update visited list\n",
        "    for item in pair_keys:\n",
        "        visited.append(item)\n",
        "\n",
        "pipeline = pipeline.to(\"cuda\")\n",
        "pipeline.safety_checker = lambda images, clip_input: (images, False)\n",
        "\n",
        "prompt =\"a girl in dress\"\n",
        "with torch.no_grad():\n",
        "    image = pipeline(prompt=prompt,\n",
        "                     height=1280, \n",
        "                     width=720,\n",
        "                     num_inference_steps=50,\n",
        "                     guidance_scale=8).images[0]\n",
        "\n",
        "image.save(\"./{}_{}.png\".format(prompt[:50],alpha))"
      ],
      "metadata": {
        "id": "NUKfQXNPQ8aS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
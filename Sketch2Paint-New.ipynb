{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e7618a51-5f7e-4978-ac53-463e26302372",
   "metadata": {},
   "source": [
    "## Environment Set-up üòÄ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a9dbe17-fb16-42b1-9d49-95a54bac2fcb",
   "metadata": {},
   "source": [
    "You have to run it with Nvidia GPU, though..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebab20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # !conda activate aidraw\n",
    "# !pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a6efa9-db75-431c-b352-ac9d4c24a073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade --force-reinstall diffusers==0.14.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e349bff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade --force-reinstall transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ee468a-e043-4554-b189-3da2090fc486",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install opencv-contrib-python\n",
    "# !pip install controlnet_aux\n",
    "# !pip install requests==2.28.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c2a288-9719-41a4-b917-e104f0685a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b7331f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio==0.11.0 --user --extra-index-url https://download.pytorch.org/whl/cu113"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce6baf85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "827b6611",
   "metadata": {},
   "source": [
    "## 0-to-Sketch LoRA (Deprycated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bb88ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import requests \n",
    "# from PIL import Image \n",
    "# import gradio as gr \n",
    "# from io import BytesIO \n",
    "# url = \"https://stablediffusionapi.com/api/v3/text2img\" \n",
    "# title = \"\"\"&lt;h2&gt;&lt;center&gt;Text to Image Generation with Stable Diffusion API&lt;/center&gt;&lt;/h2&gt;\"\"\" \n",
    "# description = \"\"\"#### Get the API key by signing up here [Stable Diffusion API](https://stablediffusionapi.com).\"\"\" \n",
    "# def get_image(key, prompt, inference_steps, filter): \n",
    "#     payload = { \"key\": key, \"prompt\": prompt, \"negative_prompt\": \"((out of frame)), ((extra fingers)), mutated hands, ((poorly drawn hands)), ((poorly drawn face)), (((mutation))), (((deformed))), (((tiling))), ((naked)), ((tile)), ((fleshpile)), ((ugly)), (((abstract))), blurry, ((bad anatomy)), ((bad proportions)), ((extra limbs)), cloned face, (((skinny))), glitchy, ((extra breasts)), ((double torso)), ((extra arms)), ((extra hands)), ((mangled fingers)), ((missing breasts)), (missing lips), ((ugly face)), ((fat)), ((extra legs)), anime\", \"width\": \"512\", \"height\": \"512\", \"samples\": \"1\", \"num_inference_steps\": inference_steps,\"safety_checker\": filter,\"enhance_prompt\": \"yes\",\"guidance_scale\": 7.5} \n",
    "#     headers = {} \n",
    "#     response = requests.request(\"POST\", url, headers=headers, data=payload) \n",
    "#     url1 = str(json.loads(response.text)['output'][0]) \n",
    "#     r = requests.get(url1) \n",
    "#     i = Image.open(BytesIO(r.content)) \n",
    "#     return i \n",
    "# demo = gr.Interface(fn=get_image, inputs = [gr.Textbox(label=\"Enter API key\"), \n",
    "#                                             gr.Textbox(label=\"Enter the Prompt\"), gr.Number(label=\"Enter number of steps\"),gr.Checkbox(label=\"Safety filter\")], outputs = gr.Image(type='pil'), title = title, description = description)\n",
    "# demo.launch(debug='True')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2dbf1ef7-023c-4aad-b844-d31503ca5a26",
   "metadata": {},
   "source": [
    "## ControlNet Pipeline üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e203dee-4cd6-478f-8979-711c4b9fb1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\n",
    "import torch\n",
    "import numpy as np\n",
    "from diffusers.utils import load_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0154852b-5217-4f9d-923c-e30ade53bd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Reshape the Image to fit ControlNet model input (512*512)\n",
    "def re_image(image=None):\n",
    "    # Check the image\n",
    "    if image is None:\n",
    "        print(\"Error: No image.\")\n",
    "        return\n",
    "\n",
    "    # Convert the image to a numpy array if it isn't already\n",
    "    if not isinstance(image, np.ndarray):\n",
    "        img = np.array(image)\n",
    "    else:\n",
    "        img = image\n",
    "\n",
    "    # Preparing Canvas\n",
    "    height, width, _ = img.shape\n",
    "    longer_side = max(height, width)\n",
    "    square_canvas = 255 * np.ones((longer_side, longer_side, 3), dtype=np.uint8)\n",
    "\n",
    "    y_offset = (longer_side - height) // 2\n",
    "    x_offset = (longer_side - width) // 2\n",
    "\n",
    "    square_canvas[y_offset:y_offset + height, x_offset:x_offset + width] = img\n",
    "\n",
    "    # Resize the image to 512x512 pixels\n",
    "    resized_img = cv2.resize(square_canvas, (512, 512), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    return resized_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d748406-d303-4273-8e8c-5580ceac0534",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe1 = None\n",
    "def init_Sketch2Paint():\n",
    "\n",
    "    global pipe1\n",
    "    controlnet = ControlNetModel.from_pretrained(\n",
    "        \"lllyasviel/sd-controlnet-canny\", torch_dtype=torch.float16\n",
    "    )\n",
    "\n",
    "    pipe1 = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "        \"Linaqruf/anything-v3.0\", controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16\n",
    "    )\n",
    "\n",
    "    pipe1.scheduler = UniPCMultistepScheduler.from_config(pipe1.scheduler.config)\n",
    "\n",
    "    # Remove if you do not have xformers installed\n",
    "    # see https://huggingface.co/docs/diffusers/v0.13.0/en/optimization/xformers#installing-xformers\n",
    "    # for installation instructions\n",
    "    # pipe.enable_xformers_memory_efficient_attention()\n",
    "\n",
    "    pipe1.enable_model_cpu_offload()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6698c64-989e-438c-9ee6-1b1165b87b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting sketch-to-paint process...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\wz148\\AppData\\Local\\anaconda3\\lib\\site-packages\\gradio\\routes.py\", line 422, in run_predict\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"C:\\Users\\wz148\\AppData\\Local\\anaconda3\\lib\\site-packages\\gradio\\blocks.py\", line 1323, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"C:\\Users\\wz148\\AppData\\Local\\anaconda3\\lib\\site-packages\\gradio\\blocks.py\", line 1051, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"C:\\Users\\wz148\\AppData\\Local\\anaconda3\\lib\\site-packages\\anyio\\to_thread.py\", line 28, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(func, *args, cancellable=cancellable,\n",
      "  File \"C:\\Users\\wz148\\AppData\\Local\\anaconda3\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 818, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"C:\\Users\\wz148\\AppData\\Local\\anaconda3\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 754, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"C:\\Users\\wz148\\AppData\\Local\\Temp\\ipykernel_21220\\1575128515.py\", line 15, in sketch2paint\n",
      "    image1 = pipe1(text, image, num_inference_steps=int(num),seed=-1).images[0]\n",
      "  File \"C:\\Users\\wz148\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\autograd\\grad_mode.py\", line 27, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "TypeError: StableDiffusionControlNetPipeline.__call__() got an unexpected keyword argument 'seed'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting sketch-to-paint process...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\wz148\\AppData\\Local\\anaconda3\\lib\\site-packages\\gradio\\routes.py\", line 422, in run_predict\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"C:\\Users\\wz148\\AppData\\Local\\anaconda3\\lib\\site-packages\\gradio\\blocks.py\", line 1323, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"C:\\Users\\wz148\\AppData\\Local\\anaconda3\\lib\\site-packages\\gradio\\blocks.py\", line 1051, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"C:\\Users\\wz148\\AppData\\Local\\anaconda3\\lib\\site-packages\\anyio\\to_thread.py\", line 28, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(func, *args, cancellable=cancellable,\n",
      "  File \"C:\\Users\\wz148\\AppData\\Local\\anaconda3\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 818, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"C:\\Users\\wz148\\AppData\\Local\\anaconda3\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 754, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"C:\\Users\\wz148\\AppData\\Local\\Temp\\ipykernel_21220\\1575128515.py\", line 15, in sketch2paint\n",
      "    image1 = pipe1(text, image, num_inference_steps=int(num),seed=-1).images[0]\n",
      "  File \"C:\\Users\\wz148\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\autograd\\grad_mode.py\", line 27, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "TypeError: StableDiffusionControlNetPipeline.__call__() got an unexpected keyword argument 'seed'\n"
     ]
    }
   ],
   "source": [
    "def sketch2paint(img,text,num=20):\n",
    "    image_re = re_image(image=img)\n",
    "    image_re0 =Image.fromarray(image_re)\n",
    "\n",
    "    image = np.array(image_re0)\n",
    "    low_threshold = 150\n",
    "    high_threshold = 200\n",
    "\n",
    "    image = cv2.Canny(image, low_threshold, high_threshold)\n",
    "    image = image[:, :, None]\n",
    "    image = np.concatenate([image, image, image], axis=2)\n",
    "    image = Image.fromarray(image)\n",
    "    print(\"starting sketch-to-paint process...\")\n",
    "    global pipe1\n",
    "    image1 = pipe1(text, image, num_inference_steps=int(num)).images[0] #generator=torch.manual_seed(seed)\n",
    "    image1.save(\"Colorization_output.png\")\n",
    "    \n",
    "    return image1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5b8e83c6-e7ef-4457-b392-e1ca02a81483",
   "metadata": {},
   "source": [
    "## Use it With GUI! üò∫"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4049872c-e4ed-4ee2-bdb1-fc6cc8133c8d",
   "metadata": {},
   "source": [
    "### Start Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0529be8-908f-42d1-94e7-520d9023c83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vae\\diffusion_pytorch_model.safetensors not found\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "744893c9106b45faaa7fcdef311a8481",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 15 files:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "init_Sketch2Paint()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "697f6dab-23db-443d-b29e-3b9104089469",
   "metadata": {},
   "source": [
    "### Start GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ec4437f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting sketch-to-paint process...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9779f356c2448bd96fdb57635cb9dec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting sketch-to-paint process...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2c3aac797954a6f9c3093f4cbe8203b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting sketch-to-paint process...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53a8a224a93c461aac0079539e6b7e51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gradio as gr\n",
    "\n",
    "# def generate(img): #fake generate\n",
    "#     return img\n",
    "css='''\n",
    ".container {max-width: 1150px;margin: auto;padding-top: 1.5rem}\n",
    ".image_upload{min-height:500px}\n",
    ".image_upload [data-testid=\"image\"], .image_upload [data-testid=\"image\"] > div{min-height: 500px}\n",
    ".image_upload [data-testid=\"sketch\"], .image_upload [data-testid=\"sketch\"] > div{min-height: 500px}\n",
    ".image_upload .touch-none{display: flex}\n",
    "#output_image{min-height:500px;max-height=500px;}\n",
    "'''\n",
    "\n",
    "example_button=gr.Button(label='example',value='Use Result as New Input').style(full_width=False, size='sm')\n",
    "def example_fill1():\n",
    "\n",
    "  return Image.open('Sketch_1.jpeg')\n",
    "\n",
    "\n",
    "\n",
    "with gr.Blocks(theme=gr.themes.Default(font=[gr.themes.GoogleFont(\"IBM Plex Mono\"), \"ui-monospace\",\"monospace\"],\n",
    "                                       primary_hue=\"lime\",\n",
    "                                       secondary_hue=\"emerald\",\n",
    "                                       neutral_hue=\"slate\",\n",
    "                                       ), css=css) as demo:\n",
    "    with gr.Accordion('Â∞èÊèêÁ§∫‚ú®', open=False):\n",
    "        gr.Markdown('1. ËØ∑Ê†πÊçÆÊèèËø∞Ëá™Ë°åÊãÜÂàÜÂíåÊ∑ªÂä†ÂÖ≥ÈîÆËØç')\n",
    "        gr.Markdown('2. ÂÖ≥ÈîÆËØçÊèèËø∞ÂâçÂÖàÂä†ÂÖ•best quality, masterpieceÁ≠âËØç‰ª•‰øùËØÅÁîüÊàêË¥®Èáè')\n",
    "        gr.Markdown('3. ÂÖ≥ÈîÆËØçÊ∑ªÂä†È°∫Â∫èÔºöÂÖàÊèèËø∞‰∫∫ÔºàÊàñËÄÖ‰∏ª‰ΩìÔºâÔºåÂêéÊèèËø∞Áâ©')\n",
    "        gr.Markdown('4. ÈúÄË¶ÅËã±ÊñáÁøªËØëÊó∂ÂèØ‰ª•ËØ¢ÈóÆÈááËÆøËÄÖ~')\n",
    "        gr.Markdown('5. Âè≥ÈîÆÂçïÂáªÂõæÂÉè‰øùÂ≠òÂà∞Êú¨Âú∞')\n",
    "    with gr.Row().style(equal_height=True):\n",
    "        with gr.Column():\n",
    "            gr.Markdown('### ËæìÂÖ•Á∫øÁ®øÂíåÊñáÂ≠ó')\n",
    "            example_button=gr.Button(value=\"Áî®ÂàöÂàöÊîπÂ•ΩÁöÑÁ∫øÁ®øÂêßÔºÅ\")\n",
    "            input_image = gr.Image(shape=(200, 200))\n",
    "            input_text = gr.Textbox(label=\"‰∏äËâ≤ÊñáÂ≠óÊèèËø∞\",lines=1)\n",
    "            generate_button = gr.Button(label=\"‰∏äËâ≤üéá\", value = \"‰∏äËâ≤üéá\")\n",
    "            \n",
    "\n",
    "        with gr.Column():  \n",
    "            gr.Markdown('### ‰∏äËâ≤ÁªìÊûú')   \n",
    "            output_image = gr.Image() \n",
    "            \n",
    "    \n",
    "\n",
    "    generate_button.click(fn=sketch2paint, inputs =[input_image, input_text], outputs=[output_image])\n",
    "    example_button.click(fn=example_fill1, outputs=[input_image])        \n",
    "# demo = gr.Interface(sketch2paint, [gr.Image(shape=(200, 200)),gr.Textbox(lines=1)], \"image\",css=css)\n",
    "\n",
    "demo.queue().launch()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4bf7fe3e",
   "metadata": {},
   "source": [
    "### New Feature\n",
    "#### Resize the sketch input freely\n",
    "- with canva size fixed (512*512)\n",
    "   - can be flexible in future\n",
    "- resize with gradio UI easily\n",
    "- give more space to background image generation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d01582de",
   "metadata": {},
   "source": [
    "## ControlNet Inpaint Demo (Experimental)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33a5b14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c0e8f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "safety_checker\\model.safetensors not found\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "743768d997184ebfa297bae83f902cb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 16 files:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n"
     ]
    }
   ],
   "source": [
    "# this code is largely inspired by https://huggingface.co/spaces/hysts/ControlNet-with-Anything-v4/blob/main/app_scribble_interactive.py\n",
    "# Thank you, hysts!\n",
    "\n",
    "# import sys\n",
    "# sys.path.append('./src/ControlNetInpaint/')\n",
    "# functionality based on https://github.com/mikonvergence/ControlNetInpaint\n",
    "\n",
    "import gradio as gr\n",
    "#import torch\n",
    "#from torch import autocast // only for GPU\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "import os\n",
    "\n",
    "# Usage\n",
    "# 1. Upload image or fill with white\n",
    "# 2. Sketch the mask (image->[image,mask]\n",
    "# 3. Sketch the content of the mask\n",
    "\n",
    "## SETUP PIPE\n",
    "\n",
    "from diffusers import StableDiffusionInpaintPipeline, ControlNetModel, UniPCMultistepScheduler\n",
    "from pipeline_stable_diffusion_controlnet_inpaint import *\n",
    "from diffusers.utils import load_image\n",
    "from controlnet_aux import HEDdetector\n",
    "\n",
    "hed = HEDdetector.from_pretrained('lllyasviel/ControlNet')\n",
    "\n",
    "controlnet = ControlNetModel.from_pretrained(\n",
    "    \"fusing/stable-diffusion-v1-5-controlnet-scribble\", torch_dtype=torch.float16\n",
    ")\n",
    "pipe = StableDiffusionControlNetInpaintPipeline.from_pretrained(\n",
    "     \"runwayml/stable-diffusion-inpainting\", controlnet=controlnet, torch_dtype=torch.float16\n",
    " )\n",
    "\n",
    "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "pipe.safety_checker = lambda images, clip_input: (images, False)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # Remove if you do not have xformers installed\n",
    "    # see https://huggingface.co/docs/diffusers/v0.13.0/en/optimization/xformers#installing-xformers\n",
    "    # for installation instructions\n",
    "#     pipe.enable_xformers_memory_efficient_attention()\n",
    "\n",
    "    pipe.to('cuda')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7978f41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "css='''\n",
    ".container {max-width: 1150px;margin: auto;padding-top: 1.5rem}\n",
    ".image_upload{min-height:500px}\n",
    ".image_upload [data-testid=\"image\"], .image_upload [data-testid=\"image\"] > div{min-height: 500px}\n",
    ".image_upload [data-testid=\"sketch\"], .image_upload [data-testid=\"sketch\"] > div{min-height: 500px}\n",
    ".image_upload .touch-none{display: flex}\n",
    "#output_image{min-height:500px;max-height=500px;}\n",
    "'''\n",
    " \n",
    "def get_guide(image):  \n",
    "    return hed(image,scribble=True)\n",
    "\n",
    "def create_demo():\n",
    "    # Global Storage\n",
    "    CURRENT_IMAGE={'image': None,\n",
    "                   'mask': None,\n",
    "                   'guide': None\n",
    "                }    \n",
    "    HEIGHT, WIDTH=512,512\n",
    "    \n",
    "    RESULT_IMAGE=None\n",
    "    \n",
    "    with gr.Blocks(theme=gr.themes.Default(font=[gr.themes.GoogleFont(\"IBM Plex Mono\"), \"ui-monospace\",\"monospace\"],\n",
    "                                           primary_hue=\"lime\",\n",
    "                                           secondary_hue=\"emerald\",\n",
    "                                           neutral_hue=\"slate\",\n",
    "                                           ),css=css) as demo:\n",
    "\n",
    "        gr.Markdown('# Á∫øÁ®ø‰øÆÊîπ‚úèÔ∏è')\n",
    "#         with gr.Accordion('Useful Buttons', open=False):\n",
    "        with gr.Accordion('Â∞èÊèêÁ§∫‚ú®', open=False):\n",
    "            gr.Markdown('1. ËØ∑Ê†πÊçÆÊèèËø∞Ëá™Ë°åÊãÜÂàÜÂíåÊ∑ªÂä†ÂÖ≥ÈîÆËØç')\n",
    "            gr.Markdown('2. ÂÖ≥ÈîÆËØçÊèèËø∞ÂâçÂÖàÂä†ÂÖ•sketchÔºålineartÁ≠âËØç‰øùËØÅÁîüÊàêÁªìÊûú‰∏∫Á∫øÁ®øÔºåÂä†ÂÖ•best quality, masterpieceÁ≠âËØç‰ª•‰øùËØÅÁîüÊàêË¥®Èáè')\n",
    "            gr.Markdown('3. ÂÖ≥ÈîÆËØçÊ∑ªÂä†È°∫Â∫èÔºöÂÖàÊèèËø∞‰∫∫ÔºàÊàñËÄÖ‰∏ª‰ΩìÔºâÔºåÂêéÊèèËø∞Áâ©')\n",
    "            gr.Markdown('4. ÈúÄË¶ÅËã±ÊñáÁøªËØëÊó∂ÂèØ‰ª•ËØ¢ÈóÆÈááËÆøËÄÖ~')\n",
    "            gr.Markdown('5. Âè≥ÈîÆÂçïÂáªÂõæÂÉè‰øùÂ≠òÂà∞Êú¨Âú∞')\n",
    "            gr.Markdown('PSÔºöÁ∫øÁ®ø‰øÆÊîπÁªìÊûúÂèØËÉΩÂê´ÊúâÈÉ®ÂàÜËâ≤ÂΩ©Ôºå‰ΩÜ‰∏ç‰ºöÂΩ±Âìç‰πãÂêéÁöÑ‰∏äËâ≤ËøáÁ®ã')\n",
    "            gr.Markdown('PPSÔºöËøõË°åÈáçÊñ∞‰øÆÊîπÊó∂ËØ∑Âà∑Êñ∞ÁΩëÈ°µ~')\n",
    "        with gr.Box():\n",
    "#             gr.Markdown('## Cut ‚úÇÔ∏è')\n",
    "#             gr.Markdown('1. Upload your image below')\n",
    "#             gr.Markdown('2. **Draw the mask** for the region you want changed (Cut ‚úÇÔ∏è)')\n",
    "#             gr.Markdown('3. Click `Set Mask` when it is ready!')\n",
    "#             gr.Markdown('## Sketch ‚úèÔ∏è')\n",
    "#             gr.Markdown('4. Now, you can **sketch a replacement** object! (Sketch ‚úèÔ∏è)')\n",
    "#             gr.Markdown('5. (You can also provide a **text prompt** if you want)')\n",
    "#             gr.Markdown('6. üîÆ Click `Generate` when ready! ')             \n",
    "            example_button=gr.Button(label='example',value='ÁªßÁª≠Áî®‰∏ä‰∏ÄÊ¨°ÁöÑ‰øÆÊîπüòä').style(full_width=False, size='sm')\n",
    "#             restart_button = gr.Button(label='restart', value = \"Restart Everything\").style(full_width=False, size='sm')\n",
    "            \n",
    "        with gr.Group():\n",
    "          with gr.Box():\n",
    "            with gr.Column():\n",
    "              with gr.Row() as main_blocks:\n",
    "                  with gr.Column() as step_1:\n",
    "                      gr.Markdown('### ËíôÁâàËæìÂÖ•')   \n",
    "                      image = gr.Image(source='upload',\n",
    "                                            shape=[HEIGHT,WIDTH],\n",
    "                                            type='pil',#numpy',\n",
    "                                            elem_classes=\"image_upload\",\n",
    "                                       label='Mask Draw (Cut!)',\n",
    "                                            tool='sketch',\n",
    "                                            brush_radius=60).style(height=500)\n",
    "                      input_image=image\n",
    "                      mask_button = gr.Button(label='Set Mask', value='ËÆæÁΩÆËíôÁâà')\n",
    "                  with gr.Column(visible=False) as step_2:     \n",
    "                      gr.Markdown('### ËçâÁ®øËæìÂÖ•')         \n",
    "                      sketch = gr.Image(source='upload',\n",
    "                                            shape=[HEIGHT,WIDTH],\n",
    "                                            type='pil',#'numpy',\n",
    "                                        elem_classes=\"image_upload\",\n",
    "                                            label='Áîª‰∏äÊñ∞‰∏úË•øÔºÅ',#Fill Draw (Sketch!)\n",
    "                                            tool='sketch',\n",
    "                                            brush_radius=10).style(height=500)\n",
    "                      sketch_image=sketch\n",
    "                      run_button = gr.Button(label='Generate', value='ÁîüÊàê‰øÆÊîπÁªìÊûúüéá', variant=\"primary\") \n",
    "                      prompt = gr.Textbox(label='Prompt')    \n",
    "              \n",
    "                  with gr.Column() as output_step:  \n",
    "                      gr.Markdown('### ‰øÆÊîπÁªìÊûú')   \n",
    "                      output_image = gr.Gallery(\n",
    "                                      label=\"Generated images\",\n",
    "                                      show_label=False,\n",
    "                                      elem_id=\"output_image\",\n",
    "                                  ).style(height=500,containter=True)              \n",
    "              with gr.Accordion('Advanced options', open=False):\n",
    "                  num_steps = gr.Slider(label='Steps',\n",
    "                                      minimum=1,\n",
    "                                      maximum=100,\n",
    "                                      value=20,\n",
    "                                      step=1)\n",
    "                  text_scale = gr.Slider(label='Text Guidance Scale',\n",
    "                                            minimum=0.1,\n",
    "                                            maximum=30.0,\n",
    "                                            value=7.5,\n",
    "                                            step=0.1)\n",
    "                  seed = gr.Slider(label='Seed',\n",
    "                                  minimum=-1,\n",
    "                                  maximum=2147483647,\n",
    "                                  step=1,\n",
    "                                  randomize=True)  \n",
    "                  \n",
    "                  sketch_scale = gr.Slider(label='Sketch Guidance Scale',\n",
    "                                            minimum=0.0,\n",
    "                                            maximum=1.0,\n",
    "                                            value=1.0,\n",
    "                                            step=0.05)\n",
    "                    \n",
    "\n",
    "#         with gr.Accordion('More Info', open=False):\n",
    "#             gr.Markdown('This demo was created by Mikolaj Czerkawski [@mikonvergence](https://twitter.com/mikonvergence) based on the üå± open-source implementation of [ControlNetInpaint](https://github.com/mikonvergence/ControlNetInpaint) (diffusers-friendly!).')\n",
    "#             gr.Markdown('The tool currently only works with image resolution of 512px.')\n",
    "#             gr.Markdown('üí° To learn more about diffusion with interactive code, check out my open-source ‚è©[DiffusionFastForward](https://github.com/mikonvergence/DiffusionFastForward) course. It contains example code, executable notebooks, videos, notes, and a few use cases for training from scratch!')\n",
    "        \n",
    "        inputs = [\n",
    "            sketch_image,\n",
    "            prompt,\n",
    "            num_steps,\n",
    "            text_scale,\n",
    "            sketch_scale,\n",
    "            seed\n",
    "        ]\n",
    "        \n",
    "        # STEP 1: Set Mask\n",
    "        def set_mask(content):\n",
    "            if content is None:\n",
    "              gr.Error(\"You must upload an image first.\")\n",
    "              return {input_image : None,\n",
    "                      sketch_image : None,\n",
    "                      step_1: gr.update(visible=True),\n",
    "                      step_2: gr.update(visible=False)\n",
    "                  }\n",
    "                  \n",
    "            background=np.array(content[\"image\"].convert(\"RGB\").resize((512, 512))) # note: direct numpy seemed buggy\n",
    "            mask=np.array(content[\"mask\"].convert(\"RGB\").resize((512, 512)))\n",
    "\n",
    "            if (mask==0).all():\n",
    "              gr.Error(\"You must draw a mask for the cut out first.\")\n",
    "              return {input_image : content['image'],\n",
    "                      sketch_image : None,\n",
    "                      step_1: gr.update(visible=True),\n",
    "                      step_2: gr.update(visible=False)\n",
    "                  }\n",
    "\n",
    "            mask=1*(mask>0)\n",
    "            # save vars\n",
    "            CURRENT_IMAGE['image']=background\n",
    "            CURRENT_IMAGE['mask']=mask\n",
    "            \n",
    "            guide=get_guide(background)\n",
    "            CURRENT_IMAGE['guide']=np.array(guide)\n",
    "            guide=255-np.asarray(guide)  \n",
    "            \n",
    "            seg_img = guide*(1-mask) + mask*192\n",
    "            preview = background * (seg_img==255)\n",
    "            \n",
    "            vis_image=(preview/2).astype(seg_img.dtype) + seg_img * (seg_img!=255)\n",
    "            \n",
    "            return {input_image : content[\"image\"],\n",
    "                    sketch_image : vis_image,\n",
    "                    step_1: gr.update(visible=False),\n",
    "                    step_2: gr.update(visible=True)\n",
    "                }\n",
    "        \n",
    "        # STEP 2: Generate\n",
    "        def generate(content,\n",
    "             prompt,\n",
    "             num_steps,\n",
    "             text_scale,\n",
    "             sketch_scale,\n",
    "             seed):\n",
    "            sketch=np.array(content[\"mask\"].convert(\"RGB\").resize((512, 512)))            \n",
    "            sketch=(255*(sketch>0)).astype(CURRENT_IMAGE['image'].dtype) \n",
    "            mask=CURRENT_IMAGE['mask']\n",
    "            \n",
    "            CURRENT_IMAGE['guide']=(CURRENT_IMAGE['guide']*(mask==0) + sketch*(mask!=0)).astype(CURRENT_IMAGE['image'].dtype)\n",
    "            \n",
    "            mask_img=255*CURRENT_IMAGE['mask'].astype(CURRENT_IMAGE['image'].dtype)\n",
    "            \n",
    "            new_image = pipe(\n",
    "              prompt,\n",
    "              num_inference_steps=num_steps,\n",
    "              guidance_scale=text_scale,\n",
    "              generator=torch.manual_seed(seed),\n",
    "              image=Image.fromarray(CURRENT_IMAGE['image']),\n",
    "              control_image=Image.fromarray(CURRENT_IMAGE['guide']),\n",
    "              controlnet_conditioning_scale=sketch_scale,\n",
    "              mask_image=Image.fromarray(mask_img)\n",
    "            ).images#[0]\n",
    "            \n",
    "            RESULT_IMAGE = new_image\n",
    "            \n",
    "            RESULT_IMAGE[0].save(\"Sketch_1.jpeg\")\n",
    "            \n",
    "            return {output_image : new_image,\n",
    "                step_1: gr.update(visible=True),\n",
    "                step_2: gr.update(visible=False)\n",
    "                }\n",
    "\n",
    "        def example_fill():\n",
    "\n",
    "          return Image.open('Sketch_1.jpeg')\n",
    "#             if RESULT_IMAGE != None:\n",
    "#                 print(\"not a valid image\")\n",
    "#             return RESULT_IMAGE\n",
    "        \n",
    "        example_button.click(fn=example_fill, outputs=[input_image])\n",
    "        mask_button.click(fn=set_mask, inputs=[input_image], outputs=[input_image, sketch_image, step_1,step_2])     \n",
    "        run_button.click(fn=generate, inputs=inputs, outputs=[output_image, step_1,step_2])\n",
    "        return demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89a9e798",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wz148\\AppData\\Local\\anaconda3\\lib\\site-packages\\gradio\\components.py:164: UserWarning: Unknown style parameter: containter\n",
      "  warnings.warn(f\"Unknown style parameter: {key}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    demo = create_demo()\n",
    "    demo.queue().launch() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09d7b21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gradio as gr\n",
    "\n",
    "# def file_selected(file_input):\n",
    "#     print(\"yes, file_selected is invoked\")\n",
    "#     print(process_button)\n",
    "#     return gr.update(visible=True)\n",
    "    \n",
    "# with gr.Blocks() as demo:\n",
    "#     with gr.Row():\n",
    "#         with gr.Column(scale=1):\n",
    "#             gr.Markdown(\"### Data\")\n",
    "#             file_input = gr.File(label=\"Select File\")\n",
    "#             process_button = gr.Button(\"Process\", visible=False)\n",
    "\n",
    "#         with gr.Column(scale=2, min_width=600):\n",
    "#             gr.Markdown(\"### Output\")\n",
    "#             result_display = gr.TextArea(default=\"\", label=\"Result\", lines=10, visible=False)\n",
    "\n",
    "#     file_input.change(fn=file_selected, inputs=file_input, outputs=process_button)\n",
    "    \n",
    "# if __name__ == \"__main__\":\n",
    "#     demo.launch()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ea726a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

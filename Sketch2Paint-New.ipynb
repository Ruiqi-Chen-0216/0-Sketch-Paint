{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e7618a51-5f7e-4978-ac53-463e26302372",
   "metadata": {},
   "source": [
    "## Environment Set-up 😀"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a9dbe17-fb16-42b1-9d49-95a54bac2fcb",
   "metadata": {},
   "source": [
    "You have to run it with Nvidia GPU, though..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebab20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # !conda activate aidraw\n",
    "# !pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a6efa9-db75-431c-b352-ac9d4c24a073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade --force-reinstall diffusers==0.14.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e349bff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade --force-reinstall transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ee468a-e043-4554-b189-3da2090fc486",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install opencv-contrib-python\n",
    "# !pip install controlnet_aux\n",
    "# !pip install requests==2.28.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c2a288-9719-41a4-b917-e104f0685a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b7331f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio==0.11.0 --user --extra-index-url https://download.pytorch.org/whl/cu113"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce6baf85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "827b6611",
   "metadata": {},
   "source": [
    "## 0-to-Sketch LoRA (Deprycated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bb88ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import requests \n",
    "# from PIL import Image \n",
    "# import gradio as gr \n",
    "# from io import BytesIO \n",
    "# url = \"https://stablediffusionapi.com/api/v3/text2img\" \n",
    "# title = \"\"\"&lt;h2&gt;&lt;center&gt;Text to Image Generation with Stable Diffusion API&lt;/center&gt;&lt;/h2&gt;\"\"\" \n",
    "# description = \"\"\"#### Get the API key by signing up here [Stable Diffusion API](https://stablediffusionapi.com).\"\"\" \n",
    "# def get_image(key, prompt, inference_steps, filter): \n",
    "#     payload = { \"key\": key, \"prompt\": prompt, \"negative_prompt\": \"((out of frame)), ((extra fingers)), mutated hands, ((poorly drawn hands)), ((poorly drawn face)), (((mutation))), (((deformed))), (((tiling))), ((naked)), ((tile)), ((fleshpile)), ((ugly)), (((abstract))), blurry, ((bad anatomy)), ((bad proportions)), ((extra limbs)), cloned face, (((skinny))), glitchy, ((extra breasts)), ((double torso)), ((extra arms)), ((extra hands)), ((mangled fingers)), ((missing breasts)), (missing lips), ((ugly face)), ((fat)), ((extra legs)), anime\", \"width\": \"512\", \"height\": \"512\", \"samples\": \"1\", \"num_inference_steps\": inference_steps,\"safety_checker\": filter,\"enhance_prompt\": \"yes\",\"guidance_scale\": 7.5} \n",
    "#     headers = {} \n",
    "#     response = requests.request(\"POST\", url, headers=headers, data=payload) \n",
    "#     url1 = str(json.loads(response.text)['output'][0]) \n",
    "#     r = requests.get(url1) \n",
    "#     i = Image.open(BytesIO(r.content)) \n",
    "#     return i \n",
    "# demo = gr.Interface(fn=get_image, inputs = [gr.Textbox(label=\"Enter API key\"), \n",
    "#                                             gr.Textbox(label=\"Enter the Prompt\"), gr.Number(label=\"Enter number of steps\"),gr.Checkbox(label=\"Safety filter\")], outputs = gr.Image(type='pil'), title = title, description = description)\n",
    "# demo.launch(debug='True')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2dbf1ef7-023c-4aad-b844-d31503ca5a26",
   "metadata": {},
   "source": [
    "## ControlNet Pipeline 🚀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e203dee-4cd6-478f-8979-711c4b9fb1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\n",
    "import torch\n",
    "import numpy as np\n",
    "from diffusers.utils import load_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0154852b-5217-4f9d-923c-e30ade53bd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Reshape the Image to fit ControlNet model input (512*512)\n",
    "def re_image(image=None):\n",
    "    # Check the image\n",
    "    if image is None:\n",
    "        print(\"Error: No image.\")\n",
    "        return\n",
    "\n",
    "    # Convert the image to a numpy array if it isn't already\n",
    "    if not isinstance(image, np.ndarray):\n",
    "        img = np.array(image)\n",
    "    else:\n",
    "        img = image\n",
    "\n",
    "    # Preparing Canvas\n",
    "    height, width, _ = img.shape\n",
    "    longer_side = max(height, width)\n",
    "    square_canvas = 255 * np.ones((longer_side, longer_side, 3), dtype=np.uint8)\n",
    "\n",
    "    y_offset = (longer_side - height) // 2\n",
    "    x_offset = (longer_side - width) // 2\n",
    "\n",
    "    square_canvas[y_offset:y_offset + height, x_offset:x_offset + width] = img\n",
    "\n",
    "    # Resize the image to 512x512 pixels\n",
    "    resized_img = cv2.resize(square_canvas, (512, 512), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    return resized_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d748406-d303-4273-8e8c-5580ceac0534",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe1 = None\n",
    "def init_Sketch2Paint():\n",
    "\n",
    "    global pipe1\n",
    "    controlnet = ControlNetModel.from_pretrained(\n",
    "        \"lllyasviel/sd-controlnet-canny\", torch_dtype=torch.float16\n",
    "    )\n",
    "\n",
    "    pipe1 = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "        \"Linaqruf/anything-v3.0\", controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16\n",
    "    )\n",
    "\n",
    "    pipe1.scheduler = UniPCMultistepScheduler.from_config(pipe1.scheduler.config)\n",
    "\n",
    "    # Remove if you do not have xformers installed\n",
    "    # see https://huggingface.co/docs/diffusers/v0.13.0/en/optimization/xformers#installing-xformers\n",
    "    # for installation instructions\n",
    "    # pipe.enable_xformers_memory_efficient_attention()\n",
    "\n",
    "    pipe1.enable_model_cpu_offload()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6698c64-989e-438c-9ee6-1b1165b87b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting sketch-to-paint process...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\wz148\\AppData\\Local\\anaconda3\\lib\\site-packages\\gradio\\routes.py\", line 422, in run_predict\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"C:\\Users\\wz148\\AppData\\Local\\anaconda3\\lib\\site-packages\\gradio\\blocks.py\", line 1323, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"C:\\Users\\wz148\\AppData\\Local\\anaconda3\\lib\\site-packages\\gradio\\blocks.py\", line 1051, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"C:\\Users\\wz148\\AppData\\Local\\anaconda3\\lib\\site-packages\\anyio\\to_thread.py\", line 28, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(func, *args, cancellable=cancellable,\n",
      "  File \"C:\\Users\\wz148\\AppData\\Local\\anaconda3\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 818, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"C:\\Users\\wz148\\AppData\\Local\\anaconda3\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 754, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"C:\\Users\\wz148\\AppData\\Local\\Temp\\ipykernel_21220\\1575128515.py\", line 15, in sketch2paint\n",
      "    image1 = pipe1(text, image, num_inference_steps=int(num),seed=-1).images[0]\n",
      "  File \"C:\\Users\\wz148\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\autograd\\grad_mode.py\", line 27, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "TypeError: StableDiffusionControlNetPipeline.__call__() got an unexpected keyword argument 'seed'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting sketch-to-paint process...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\wz148\\AppData\\Local\\anaconda3\\lib\\site-packages\\gradio\\routes.py\", line 422, in run_predict\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"C:\\Users\\wz148\\AppData\\Local\\anaconda3\\lib\\site-packages\\gradio\\blocks.py\", line 1323, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"C:\\Users\\wz148\\AppData\\Local\\anaconda3\\lib\\site-packages\\gradio\\blocks.py\", line 1051, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"C:\\Users\\wz148\\AppData\\Local\\anaconda3\\lib\\site-packages\\anyio\\to_thread.py\", line 28, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(func, *args, cancellable=cancellable,\n",
      "  File \"C:\\Users\\wz148\\AppData\\Local\\anaconda3\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 818, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"C:\\Users\\wz148\\AppData\\Local\\anaconda3\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 754, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"C:\\Users\\wz148\\AppData\\Local\\Temp\\ipykernel_21220\\1575128515.py\", line 15, in sketch2paint\n",
      "    image1 = pipe1(text, image, num_inference_steps=int(num),seed=-1).images[0]\n",
      "  File \"C:\\Users\\wz148\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\autograd\\grad_mode.py\", line 27, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "TypeError: StableDiffusionControlNetPipeline.__call__() got an unexpected keyword argument 'seed'\n"
     ]
    }
   ],
   "source": [
    "def sketch2paint(img,text,num=20):\n",
    "    image_re = re_image(image=img)\n",
    "    image_re0 =Image.fromarray(image_re)\n",
    "\n",
    "    image = np.array(image_re0)\n",
    "    low_threshold = 150\n",
    "    high_threshold = 200\n",
    "\n",
    "    image = cv2.Canny(image, low_threshold, high_threshold)\n",
    "    image = image[:, :, None]\n",
    "    image = np.concatenate([image, image, image], axis=2)\n",
    "    image = Image.fromarray(image)\n",
    "    print(\"starting sketch-to-paint process...\")\n",
    "    global pipe1\n",
    "    image1 = pipe1(text, image, num_inference_steps=int(num)).images[0] #generator=torch.manual_seed(seed)\n",
    "    image1.save(\"Colorization_output.png\")\n",
    "    \n",
    "    return image1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5b8e83c6-e7ef-4457-b392-e1ca02a81483",
   "metadata": {},
   "source": [
    "## Use it With GUI! 😺"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4049872c-e4ed-4ee2-bdb1-fc6cc8133c8d",
   "metadata": {},
   "source": [
    "### Start Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0529be8-908f-42d1-94e7-520d9023c83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vae\\diffusion_pytorch_model.safetensors not found\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "744893c9106b45faaa7fcdef311a8481",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 15 files:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "init_Sketch2Paint()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "697f6dab-23db-443d-b29e-3b9104089469",
   "metadata": {},
   "source": [
    "### Start GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ec4437f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting sketch-to-paint process...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9779f356c2448bd96fdb57635cb9dec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting sketch-to-paint process...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2c3aac797954a6f9c3093f4cbe8203b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting sketch-to-paint process...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53a8a224a93c461aac0079539e6b7e51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gradio as gr\n",
    "\n",
    "# def generate(img): #fake generate\n",
    "#     return img\n",
    "css='''\n",
    ".container {max-width: 1150px;margin: auto;padding-top: 1.5rem}\n",
    ".image_upload{min-height:500px}\n",
    ".image_upload [data-testid=\"image\"], .image_upload [data-testid=\"image\"] > div{min-height: 500px}\n",
    ".image_upload [data-testid=\"sketch\"], .image_upload [data-testid=\"sketch\"] > div{min-height: 500px}\n",
    ".image_upload .touch-none{display: flex}\n",
    "#output_image{min-height:500px;max-height=500px;}\n",
    "'''\n",
    "\n",
    "example_button=gr.Button(label='example',value='Use Result as New Input').style(full_width=False, size='sm')\n",
    "def example_fill1():\n",
    "\n",
    "  return Image.open('Sketch_1.jpeg')\n",
    "\n",
    "\n",
    "\n",
    "with gr.Blocks(theme=gr.themes.Default(font=[gr.themes.GoogleFont(\"IBM Plex Mono\"), \"ui-monospace\",\"monospace\"],\n",
    "                                       primary_hue=\"lime\",\n",
    "                                       secondary_hue=\"emerald\",\n",
    "                                       neutral_hue=\"slate\",\n",
    "                                       ), css=css) as demo:\n",
    "    with gr.Accordion('小提示✨', open=False):\n",
    "        gr.Markdown('1. 请根据描述自行拆分和添加关键词')\n",
    "        gr.Markdown('2. 关键词描述前先加入best quality, masterpiece等词以保证生成质量')\n",
    "        gr.Markdown('3. 关键词添加顺序：先描述人（或者主体），后描述物')\n",
    "        gr.Markdown('4. 需要英文翻译时可以询问采访者~')\n",
    "        gr.Markdown('5. 右键单击图像保存到本地')\n",
    "    with gr.Row().style(equal_height=True):\n",
    "        with gr.Column():\n",
    "            gr.Markdown('### 输入线稿和文字')\n",
    "            example_button=gr.Button(value=\"用刚刚改好的线稿吧！\")\n",
    "            input_image = gr.Image(shape=(200, 200))\n",
    "            input_text = gr.Textbox(label=\"上色文字描述\",lines=1)\n",
    "            generate_button = gr.Button(label=\"上色🎇\", value = \"上色🎇\")\n",
    "            \n",
    "\n",
    "        with gr.Column():  \n",
    "            gr.Markdown('### 上色结果')   \n",
    "            output_image = gr.Image() \n",
    "            \n",
    "    \n",
    "\n",
    "    generate_button.click(fn=sketch2paint, inputs =[input_image, input_text], outputs=[output_image])\n",
    "    example_button.click(fn=example_fill1, outputs=[input_image])        \n",
    "# demo = gr.Interface(sketch2paint, [gr.Image(shape=(200, 200)),gr.Textbox(lines=1)], \"image\",css=css)\n",
    "\n",
    "demo.queue().launch()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4bf7fe3e",
   "metadata": {},
   "source": [
    "### New Feature\n",
    "#### Resize the sketch input freely\n",
    "- with canva size fixed (512*512)\n",
    "   - can be flexible in future\n",
    "- resize with gradio UI easily\n",
    "- give more space to background image generation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d01582de",
   "metadata": {},
   "source": [
    "## ControlNet Inpaint Demo (Experimental)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33a5b14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c0e8f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "safety_checker\\model.safetensors not found\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "743768d997184ebfa297bae83f902cb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 16 files:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n"
     ]
    }
   ],
   "source": [
    "# this code is largely inspired by https://huggingface.co/spaces/hysts/ControlNet-with-Anything-v4/blob/main/app_scribble_interactive.py\n",
    "# Thank you, hysts!\n",
    "\n",
    "# import sys\n",
    "# sys.path.append('./src/ControlNetInpaint/')\n",
    "# functionality based on https://github.com/mikonvergence/ControlNetInpaint\n",
    "\n",
    "import gradio as gr\n",
    "#import torch\n",
    "#from torch import autocast // only for GPU\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "import os\n",
    "\n",
    "# Usage\n",
    "# 1. Upload image or fill with white\n",
    "# 2. Sketch the mask (image->[image,mask]\n",
    "# 3. Sketch the content of the mask\n",
    "\n",
    "## SETUP PIPE\n",
    "\n",
    "from diffusers import StableDiffusionInpaintPipeline, ControlNetModel, UniPCMultistepScheduler\n",
    "from pipeline_stable_diffusion_controlnet_inpaint import *\n",
    "from diffusers.utils import load_image\n",
    "from controlnet_aux import HEDdetector\n",
    "\n",
    "hed = HEDdetector.from_pretrained('lllyasviel/ControlNet')\n",
    "\n",
    "controlnet = ControlNetModel.from_pretrained(\n",
    "    \"fusing/stable-diffusion-v1-5-controlnet-scribble\", torch_dtype=torch.float16\n",
    ")\n",
    "pipe = StableDiffusionControlNetInpaintPipeline.from_pretrained(\n",
    "     \"runwayml/stable-diffusion-inpainting\", controlnet=controlnet, torch_dtype=torch.float16\n",
    " )\n",
    "\n",
    "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "pipe.safety_checker = lambda images, clip_input: (images, False)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # Remove if you do not have xformers installed\n",
    "    # see https://huggingface.co/docs/diffusers/v0.13.0/en/optimization/xformers#installing-xformers\n",
    "    # for installation instructions\n",
    "#     pipe.enable_xformers_memory_efficient_attention()\n",
    "\n",
    "    pipe.to('cuda')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7978f41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "css='''\n",
    ".container {max-width: 1150px;margin: auto;padding-top: 1.5rem}\n",
    ".image_upload{min-height:500px}\n",
    ".image_upload [data-testid=\"image\"], .image_upload [data-testid=\"image\"] > div{min-height: 500px}\n",
    ".image_upload [data-testid=\"sketch\"], .image_upload [data-testid=\"sketch\"] > div{min-height: 500px}\n",
    ".image_upload .touch-none{display: flex}\n",
    "#output_image{min-height:500px;max-height=500px;}\n",
    "'''\n",
    " \n",
    "def get_guide(image):  \n",
    "    return hed(image,scribble=True)\n",
    "\n",
    "def create_demo():\n",
    "    # Global Storage\n",
    "    CURRENT_IMAGE={'image': None,\n",
    "                   'mask': None,\n",
    "                   'guide': None\n",
    "                }    \n",
    "    HEIGHT, WIDTH=512,512\n",
    "    \n",
    "    RESULT_IMAGE=None\n",
    "    \n",
    "    with gr.Blocks(theme=gr.themes.Default(font=[gr.themes.GoogleFont(\"IBM Plex Mono\"), \"ui-monospace\",\"monospace\"],\n",
    "                                           primary_hue=\"lime\",\n",
    "                                           secondary_hue=\"emerald\",\n",
    "                                           neutral_hue=\"slate\",\n",
    "                                           ),css=css) as demo:\n",
    "\n",
    "        gr.Markdown('# 线稿修改✏️')\n",
    "#         with gr.Accordion('Useful Buttons', open=False):\n",
    "        with gr.Accordion('小提示✨', open=False):\n",
    "            gr.Markdown('1. 请根据描述自行拆分和添加关键词')\n",
    "            gr.Markdown('2. 关键词描述前先加入sketch，lineart等词保证生成结果为线稿，加入best quality, masterpiece等词以保证生成质量')\n",
    "            gr.Markdown('3. 关键词添加顺序：先描述人（或者主体），后描述物')\n",
    "            gr.Markdown('4. 需要英文翻译时可以询问采访者~')\n",
    "            gr.Markdown('5. 右键单击图像保存到本地')\n",
    "            gr.Markdown('PS：线稿修改结果可能含有部分色彩，但不会影响之后的上色过程')\n",
    "            gr.Markdown('PPS：进行重新修改时请刷新网页~')\n",
    "        with gr.Box():\n",
    "#             gr.Markdown('## Cut ✂️')\n",
    "#             gr.Markdown('1. Upload your image below')\n",
    "#             gr.Markdown('2. **Draw the mask** for the region you want changed (Cut ✂️)')\n",
    "#             gr.Markdown('3. Click `Set Mask` when it is ready!')\n",
    "#             gr.Markdown('## Sketch ✏️')\n",
    "#             gr.Markdown('4. Now, you can **sketch a replacement** object! (Sketch ✏️)')\n",
    "#             gr.Markdown('5. (You can also provide a **text prompt** if you want)')\n",
    "#             gr.Markdown('6. 🔮 Click `Generate` when ready! ')             \n",
    "            example_button=gr.Button(label='example',value='继续用上一次的修改😊').style(full_width=False, size='sm')\n",
    "#             restart_button = gr.Button(label='restart', value = \"Restart Everything\").style(full_width=False, size='sm')\n",
    "            \n",
    "        with gr.Group():\n",
    "          with gr.Box():\n",
    "            with gr.Column():\n",
    "              with gr.Row() as main_blocks:\n",
    "                  with gr.Column() as step_1:\n",
    "                      gr.Markdown('### 蒙版输入')   \n",
    "                      image = gr.Image(source='upload',\n",
    "                                            shape=[HEIGHT,WIDTH],\n",
    "                                            type='pil',#numpy',\n",
    "                                            elem_classes=\"image_upload\",\n",
    "                                       label='Mask Draw (Cut!)',\n",
    "                                            tool='sketch',\n",
    "                                            brush_radius=60).style(height=500)\n",
    "                      input_image=image\n",
    "                      mask_button = gr.Button(label='Set Mask', value='设置蒙版')\n",
    "                  with gr.Column(visible=False) as step_2:     \n",
    "                      gr.Markdown('### 草稿输入')         \n",
    "                      sketch = gr.Image(source='upload',\n",
    "                                            shape=[HEIGHT,WIDTH],\n",
    "                                            type='pil',#'numpy',\n",
    "                                        elem_classes=\"image_upload\",\n",
    "                                            label='画上新东西！',#Fill Draw (Sketch!)\n",
    "                                            tool='sketch',\n",
    "                                            brush_radius=10).style(height=500)\n",
    "                      sketch_image=sketch\n",
    "                      run_button = gr.Button(label='Generate', value='生成修改结果🎇', variant=\"primary\") \n",
    "                      prompt = gr.Textbox(label='Prompt')    \n",
    "              \n",
    "                  with gr.Column() as output_step:  \n",
    "                      gr.Markdown('### 修改结果')   \n",
    "                      output_image = gr.Gallery(\n",
    "                                      label=\"Generated images\",\n",
    "                                      show_label=False,\n",
    "                                      elem_id=\"output_image\",\n",
    "                                  ).style(height=500,containter=True)              \n",
    "              with gr.Accordion('Advanced options', open=False):\n",
    "                  num_steps = gr.Slider(label='Steps',\n",
    "                                      minimum=1,\n",
    "                                      maximum=100,\n",
    "                                      value=20,\n",
    "                                      step=1)\n",
    "                  text_scale = gr.Slider(label='Text Guidance Scale',\n",
    "                                            minimum=0.1,\n",
    "                                            maximum=30.0,\n",
    "                                            value=7.5,\n",
    "                                            step=0.1)\n",
    "                  seed = gr.Slider(label='Seed',\n",
    "                                  minimum=-1,\n",
    "                                  maximum=2147483647,\n",
    "                                  step=1,\n",
    "                                  randomize=True)  \n",
    "                  \n",
    "                  sketch_scale = gr.Slider(label='Sketch Guidance Scale',\n",
    "                                            minimum=0.0,\n",
    "                                            maximum=1.0,\n",
    "                                            value=1.0,\n",
    "                                            step=0.05)\n",
    "                    \n",
    "\n",
    "#         with gr.Accordion('More Info', open=False):\n",
    "#             gr.Markdown('This demo was created by Mikolaj Czerkawski [@mikonvergence](https://twitter.com/mikonvergence) based on the 🌱 open-source implementation of [ControlNetInpaint](https://github.com/mikonvergence/ControlNetInpaint) (diffusers-friendly!).')\n",
    "#             gr.Markdown('The tool currently only works with image resolution of 512px.')\n",
    "#             gr.Markdown('💡 To learn more about diffusion with interactive code, check out my open-source ⏩[DiffusionFastForward](https://github.com/mikonvergence/DiffusionFastForward) course. It contains example code, executable notebooks, videos, notes, and a few use cases for training from scratch!')\n",
    "        \n",
    "        inputs = [\n",
    "            sketch_image,\n",
    "            prompt,\n",
    "            num_steps,\n",
    "            text_scale,\n",
    "            sketch_scale,\n",
    "            seed\n",
    "        ]\n",
    "        \n",
    "        # STEP 1: Set Mask\n",
    "        def set_mask(content):\n",
    "            if content is None:\n",
    "              gr.Error(\"You must upload an image first.\")\n",
    "              return {input_image : None,\n",
    "                      sketch_image : None,\n",
    "                      step_1: gr.update(visible=True),\n",
    "                      step_2: gr.update(visible=False)\n",
    "                  }\n",
    "                  \n",
    "            background=np.array(content[\"image\"].convert(\"RGB\").resize((512, 512))) # note: direct numpy seemed buggy\n",
    "            mask=np.array(content[\"mask\"].convert(\"RGB\").resize((512, 512)))\n",
    "\n",
    "            if (mask==0).all():\n",
    "              gr.Error(\"You must draw a mask for the cut out first.\")\n",
    "              return {input_image : content['image'],\n",
    "                      sketch_image : None,\n",
    "                      step_1: gr.update(visible=True),\n",
    "                      step_2: gr.update(visible=False)\n",
    "                  }\n",
    "\n",
    "            mask=1*(mask>0)\n",
    "            # save vars\n",
    "            CURRENT_IMAGE['image']=background\n",
    "            CURRENT_IMAGE['mask']=mask\n",
    "            \n",
    "            guide=get_guide(background)\n",
    "            CURRENT_IMAGE['guide']=np.array(guide)\n",
    "            guide=255-np.asarray(guide)  \n",
    "            \n",
    "            seg_img = guide*(1-mask) + mask*192\n",
    "            preview = background * (seg_img==255)\n",
    "            \n",
    "            vis_image=(preview/2).astype(seg_img.dtype) + seg_img * (seg_img!=255)\n",
    "            \n",
    "            return {input_image : content[\"image\"],\n",
    "                    sketch_image : vis_image,\n",
    "                    step_1: gr.update(visible=False),\n",
    "                    step_2: gr.update(visible=True)\n",
    "                }\n",
    "        \n",
    "        # STEP 2: Generate\n",
    "        def generate(content,\n",
    "             prompt,\n",
    "             num_steps,\n",
    "             text_scale,\n",
    "             sketch_scale,\n",
    "             seed):\n",
    "            sketch=np.array(content[\"mask\"].convert(\"RGB\").resize((512, 512)))            \n",
    "            sketch=(255*(sketch>0)).astype(CURRENT_IMAGE['image'].dtype) \n",
    "            mask=CURRENT_IMAGE['mask']\n",
    "            \n",
    "            CURRENT_IMAGE['guide']=(CURRENT_IMAGE['guide']*(mask==0) + sketch*(mask!=0)).astype(CURRENT_IMAGE['image'].dtype)\n",
    "            \n",
    "            mask_img=255*CURRENT_IMAGE['mask'].astype(CURRENT_IMAGE['image'].dtype)\n",
    "            \n",
    "            new_image = pipe(\n",
    "              prompt,\n",
    "              num_inference_steps=num_steps,\n",
    "              guidance_scale=text_scale,\n",
    "              generator=torch.manual_seed(seed),\n",
    "              image=Image.fromarray(CURRENT_IMAGE['image']),\n",
    "              control_image=Image.fromarray(CURRENT_IMAGE['guide']),\n",
    "              controlnet_conditioning_scale=sketch_scale,\n",
    "              mask_image=Image.fromarray(mask_img)\n",
    "            ).images#[0]\n",
    "            \n",
    "            RESULT_IMAGE = new_image\n",
    "            \n",
    "            RESULT_IMAGE[0].save(\"Sketch_1.jpeg\")\n",
    "            \n",
    "            return {output_image : new_image,\n",
    "                step_1: gr.update(visible=True),\n",
    "                step_2: gr.update(visible=False)\n",
    "                }\n",
    "\n",
    "        def example_fill():\n",
    "\n",
    "          return Image.open('Sketch_1.jpeg')\n",
    "#             if RESULT_IMAGE != None:\n",
    "#                 print(\"not a valid image\")\n",
    "#             return RESULT_IMAGE\n",
    "        \n",
    "        example_button.click(fn=example_fill, outputs=[input_image])\n",
    "        mask_button.click(fn=set_mask, inputs=[input_image], outputs=[input_image, sketch_image, step_1,step_2])     \n",
    "        run_button.click(fn=generate, inputs=inputs, outputs=[output_image, step_1,step_2])\n",
    "        return demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89a9e798",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wz148\\AppData\\Local\\anaconda3\\lib\\site-packages\\gradio\\components.py:164: UserWarning: Unknown style parameter: containter\n",
      "  warnings.warn(f\"Unknown style parameter: {key}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    demo = create_demo()\n",
    "    demo.queue().launch() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09d7b21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gradio as gr\n",
    "\n",
    "# def file_selected(file_input):\n",
    "#     print(\"yes, file_selected is invoked\")\n",
    "#     print(process_button)\n",
    "#     return gr.update(visible=True)\n",
    "    \n",
    "# with gr.Blocks() as demo:\n",
    "#     with gr.Row():\n",
    "#         with gr.Column(scale=1):\n",
    "#             gr.Markdown(\"### Data\")\n",
    "#             file_input = gr.File(label=\"Select File\")\n",
    "#             process_button = gr.Button(\"Process\", visible=False)\n",
    "\n",
    "#         with gr.Column(scale=2, min_width=600):\n",
    "#             gr.Markdown(\"### Output\")\n",
    "#             result_display = gr.TextArea(default=\"\", label=\"Result\", lines=10, visible=False)\n",
    "\n",
    "#     file_input.change(fn=file_selected, inputs=file_input, outputs=process_button)\n",
    "    \n",
    "# if __name__ == \"__main__\":\n",
    "#     demo.launch()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ea726a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
